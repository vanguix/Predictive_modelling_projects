---
title: "Project 1 Predictive Modeling"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Introduction

During this project, we will analyse two data sets in order to obtain more insights about them and predict some objective variable. First, we will approach the problem with a Linear Regression model, then, with the second data set, we will try to model a Generalised Linear Model. These are the data sets used:

-   Airbnb: <https://www.kaggle.com/datasets/dipeshkhemani/airbnb-cleaned-europe-dataset>

-   Data scientist jobs: <https://www.kaggle.com/datasets/nikhilbhathi/data-scientist-salary-us-glassdoor>

# Linear Regression

## Data preparation

For the first part of the assignment we will be using the Airbnb data set. It consist of information about the Airbnb stays in some European cities. The variables are the following:

-   City: Amsterdam, Athens, Barcelona, Berlin, Budapest, Lisbon, Paris, Rome or Vienna
-   Price: continuous numeric variable
-   Day: WeekDay or Weekend
-   Room Type: Entire home/apt, Private room or Shared room
-   Shared Room: binary variable
-   Private Room: binary variable
-   Person capacity: integer from 2 to 6.
-   Superhost: binary variable
-   Multiple Rooms: binary variable
-   Business: binary variable
-   Cleanliness Rating: integer from 2 to 10.
-   Guest satisfaction: continuous numeric variable
-   Bedrooms: integer from 0 to 10
-   City center(km): continuous numeric variable
-   Metro distance(km): continuous numeric variable
-   Attraction index: continuous variable measuring the attraction to the Airbnb by the costumers
-   Normalised Atraction index: same but normalised
-   Restraunt Index: continuous variable measuring the restaurant performance index
-   Normalised Restraunt Index: same but normalised

The goal would be to predict the Price variable in terms of the others. First, it is always a good idea to separate from the beginning the training set (what the tool is going to see) from the testing set (used only to validate predictions)

```{r}
library(tidyverse)
library(MASS)
library(caret) #ML tools
library(e1071)
library(ggplot2)
```

```{r}
 
par(mar = c(2, 2, 2, 2))
# Loading and preparing data
Airbnb <- read.csv("Airbnb_data.csv")
Airbnb
```

```{r}
set.seed(123)

# split between training and testing sets
spl = createDataPartition(Airbnb$Price, p = 0.8, list = FALSE)  # 80% for training

AirbnbTrain = Airbnb[spl,]
AirbnbTest = Airbnb[-spl,]

str(AirbnbTrain)

```

Once separated, let's clean and understand the training set. First, we check any missing values. There is any missing value.

```{r}
any(is.na(AirbnbTrain))
```

Now we evaluate the numerical variables, having some statistics such as minnimum, maximum, quantiles mean and median.

```{r}
summary(AirbnbTrain)

```

From the data obtained we can obtain the following insigths.

##### Price

The maximum of price seems to be too large for the mean and median. We can get a better look with a boxplot:

```{r}
boxplot(AirbnbTrain$Price)
```

Although the maximum is an outlier, it is not alone, as we can observe many other outliers that could be representing expensive Airbnb. In the case the maximum would be alone, we could get rid of it, but in this case this can be meaningful data.

##### Attraction.Index and Restraunt.Index

As we have the normalize columns, we have redundant information, so we will only keep those.

```{r}
AirbnbTrain <- subset(AirbnbTrain, select = -c(Attraction.Index, Restraunt.Index ))

```

The rest of numerical variables seem fine with their ranges, so no changes are needed. Let's now evaluate the categorical variables:

```{r}
table(AirbnbTrain$City)
table(AirbnbTrain$Day)
table(AirbnbTrain$Room.Type)
table(AirbnbTrain$Shared.Room)
table(AirbnbTrain$Private.Room)
table(AirbnbTrain$Superhost)
table(AirbnbTrain$Multiple.Rooms)
table(AirbnbTrain$Business)

```

The variable Room type has the same information than Private Room and Shared room, so we will erase the last two.

```{r}
AirbnbTrain <- subset(AirbnbTrain, select = -c(Private.Room, Shared.Room ))

```

Finally, the variable Superhost is as False/True, we will code as binary the False/True options.

```{r}
AirbnbTrain$Superhost<-as.numeric(as.logical(AirbnbTrain$Superhost))

```

## Explanatory analysis

In order to better comprehend our data, we need to plot it. Concretely, we will have in mind plots respect to the objective variable, the price, for trying to visualize a model for predicting it.

```{r}
ggplot(AirbnbTrain, aes(Price)) + geom_density(fill="lightblue") + xlab("Price") + ggtitle("Price distribution")

## from the statistics obtained before we know price has no 0 values as minimum is 34, so we can apply the logarithm to get a better distribution

ggplot(AirbnbTrain, aes(log(Price))) + geom_density(fill="lightblue") + xlab("log(Price)") + ggtitle("Price distribution")

mean(log(AirbnbTrain$Price))
exp(mean(log(AirbnbTrain$Price)))


```

As can be observed, Price is positive asymmetric, as most of the data is located at the left of the distribution. In order to make the data more symmetric, we need to take the logarithm. The log(Price) seems to follow a Gaussian distribution with mean 5.36 (214.2107 €). This is beneficial in terms of future prediction of the target, as it is symmetrical there are no much extreme values and the regression would make better predictions. However, still after the transformation the data are not fully centered, but are slightly shifted to the left.

After observing the response variable individually, we can approximate some regressions with plots. We now have in mind the Prices in terms of the city, in order to better understand the data.

```{r}
ggplot(AirbnbTrain, aes(log(Price))) + geom_density(aes(group=City, colour=City, fill=City), alpha=0.1) + ggtitle("Price distribution")
```

Most of the cities have the same distribution, however we can observe how Berlin has the highest prices while Amsterdam the lowest.

Some of the variables that we may think are important to determine the price of an Airbnb are: distance to metro, distance to city center, number of bedrooms, restaurant and attraction indexes. Let's observe them.

```{r}
ggplot(AirbnbTrain, aes(x=City.Center..km., y=log(Price), color=City)) + geom_point(alpha=0.8) + ggtitle("Price vs City Center distance") 

```

```{r}
ggplot(AirbnbTrain, aes(x=Metro.Distance..km., y=log(Price), color=City)) + geom_point(alpha=0.8) + ggtitle("Price vs Metro distance")
```

As observed, the most expensive Airbnb are the ones really close to the metro and center (there are no points at the right upper corner).

```{r}
ggplot(AirbnbTrain, aes(x = Bedrooms, y = log(Price), fill = City)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  ggtitle("Price vs Number of bedrooms")
```

```{r}
table(AirbnbTrain$Bedrooms)
```

We could say that surprisingly, Airbnb with higher number of bedrooms are cheaper than the ones with lower number. However, we don't have enough information about the number of bedrooms higher than 4, so it is difficult to make any assumptions. From 0 to 3, the ones with 2 bedrooms seems the most expensive in general.

```{r}
ggplot(AirbnbTrain, aes(x=Normalised.Attraction.Index, y=log(Price), color=City)) + geom_point(alpha=0.8) + ggtitle("Price vs Attraction Index") 
```

```{r}
ggplot(AirbnbTrain, aes(x=Normalised.Restraunt.Index, y=log(Price), color=City)) + geom_point(alpha=0.8) + ggtitle("Price vs Restraunt Index") 
```

Finally, the indexes did not provide as much information. But, focusing on the Restaurant index we could visualize a kind of linear tendency with lower prices on the ones with lower index and higher prices close to higher index rating. However, is still difficult to make assumptions without an apropiatte model. In the following sections this relationship would be better explained.

### Correlations

The most important term when evaluating a data set for predictions are the correlations. Which are the variables that are more correlated with our objective variable? First, we would focus on isolated correlations.

```{r}
## Moving the objective function to the last position
AirbnbTrain<-AirbnbTrain[, c(1:(2 - 1), (2 + 1):ncol(AirbnbTrain), 2)]

```

```{r}
corr_delay <- sort(cor(AirbnbTrain[,c(4:15)])["Price",], decreasing = T)
corr=data.frame(corr_delay)
ggplot(corr,aes(x = row.names(corr), y = corr_delay)) + 
  geom_bar(stat = "identity", fill = "lightblue") + 
  scale_x_discrete(limits= row.names(corr)) +
  labs(x = "", y = "Price", title = "Correlations") + 
  theme(plot.title = element_text(hjust = 0, size = rel(1.5)),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

```         
```

A priori, the only variables helpful for performing a linear regression predictor are 'Normalised Atraction Index', 'Normalised Restraunt Index', 'number of Bedrooms' and 'Person Capacity'. However, is important to recall that we are plotting the individual correlations, maybe if we add information between variables we could get higher correlations, and therefore have better predictions.

### Simple linear regression: interpreting the betas

Before developing a complex model for prediction, we will try to create a simpler one where we avoid colinearities and better understand the betas. Therefore, we first create a simple linear regresor with the higher correlation variable: 'Normalised Atraction Index'.

```{r}
simple_linFit <- lm(log(Price) ~ Normalised.Attraction.Index, data=AirbnbTrain)
summary(simple_linFit)
```

The simple linear regression model is defined as: $log(Price) = \beta_0 + \beta_1 N.Attraction.Index= 5.01 + 0.03 N.Attraction.Index$

where $\beta_1$ explains that a 1 unit increase in the normalized attraction index would increase in 0.03 % the log of the Price. We can trust this model because the information gain is larger than the noise, as the p_value close to 0 confirms.

Furthermore, focusing on the global information, the residual standard error, although is larger than the local, is still not that high. The residual standard error is higher than the local because there is not much information gain from the model, however, we can trust the model as the information that $\beta_1$ gives us has small standard error.

Finally, the $R^2$ tell us about the information gain we get from the model. Almost 20%, while the remaining 80% is noise. In general, this is a difficult application, as the most correlated variable is only giving us 20% of the information, so it would be difficult to obtain high $R^2$ values on the test set. However, we always need to compare with a benchmark for better conclusions (on the following sections).

To better comprehend how this model approaches the price prediction we will plot the regression line:

```{r}
# Scatter plot

ggplot(AirbnbTrain, aes(x = Normalised.Attraction.Index, y = log(Price))) +
  geom_point(alpha = 0.8, color= 'lightblue') +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  ggtitle("Price vs Attraction Index regression")
```

After understanding the model, let's evaluate how it predicts (this result will be later compared with the benchmark and the complex linear regression model).

```{r}
pred_simple= exp(predict(simple_linFit, newdata= AirbnbTest))

#Let's know validate the predictions, calculating R^2 in the testing set

R2_simple= cor(pred_simple, log(AirbnbTest$Price))^2
R2_simple
```

As expected, $R^2$ on the testing set is smaller than on the training set (remember it was 0.2). Always is more difficult to predict future values than present ones.

Finally, we can perform some diagnosis of our simple model to obtain more conclusions:

-   Residuals vs Fitted Values: this plot show as how our model is not homoscedastic, that is, the variance of the residuals is not constant with the Price, but it increases (negatively) for higher prices. This can be explained by the non symmetric distribution and the outliers.

-   Q-Q Residuals: the residuals are not normally distributed, as the outliers in the tails of the plot suggest issues with extreme residuals.

-   Scale-Location Plot: again this plots confirm us our model is not homocedastic, as there is a clear deviation on the line.

In general, we can say our model is not good for our objective.

```{r}
par(mar = c(2, 2, 2, 2))

plot(simple_linFit, pch=23 ,bg='orange',cex=2)
```

## Predictive analysis

After understanding better how our data is constructed and being defined a simple regression model, we will know try to develop the best model for prediction. But first, we need to define the benchmark, from where we have to improve.

### Benchmark

In our case, as we are dealing with numerical continuous target, we would consider the benchmark as predicting the mean (again using the logarithmic transformation). This would be similar to the following model: $log(price)= \beta_0 \times 1$

```{r}
benchFit <- lm(log(Price) ~ 1, data=AirbnbTrain)
pred_bench <- exp(predict(benchFit, newdata=AirbnbTest))
```

As on the mean predictor the $R^2$ can not be computed, we will use the Root Mean Squared Error for comparison:

```{r}
#MSE comparison: my model vs benchmark
RMSE_simple=sqrt(mean((pred_simple - log(AirbnbTest$Price))^2))
RMSE_bench= sqrt(mean((pred_bench - log(AirbnbTest$Price))^2))
RMSE_simple
RMSE_bench
```

Clearly, our simple model is worse than the naive model, so we need to improve it.

### Complex model (without interactions)

As a first approach to find our perfect model, we would fit a model with all the variables we have. To do it, we first must change the format of the variable 'Superhost' on the test set for non finding programatic errors when calculating the predictions.

```{r}
AirbnbTest$Superhost<-as.numeric(as.logical(AirbnbTest$Superhost))

```

```{r}
all_linFit <- lm(log(Price) ~ ., data=AirbnbTrain)

# Predict on the test set
pred_all <- exp(predict(all_linFit, newdata = AirbnbTest))
  
  # Calculate MSE
RMSE_all <- sqrt(mean((pred_all - AirbnbTest$Price)^2))
R2_all= cor(pred_all, AirbnbTest$Price)^2
summary(all_linFit)
R2_all
RMSE_all
```

First, if we compare the $R^2$ values in the training and in the testing sets with the simple model, before we had 0.2 and 0.068, respectively. The value for the training set has increased until 0.66 by using all the variables, and the value on the testing set has increased on until 0.3. Of course, this complex model is better than the simpler one.

However, when comparing with the RMSE, we can see how the simple model has the worst score with 245, followed by the complex with 239, and again the benchmark is the best one with 208.

In order to try to improve this metrics, now we need to take into account interactions between variables.

### Complex model (with interactions)

We first start by doing some feature selection, as does not have any sense to add all the features because we will be adding more noise. Therefore, we would try to find the best combination of the most correlated variables: 'Normalised.Attraction.Index', 'Normalised.Restraunt.Index', 'Bedrooms' and 'Person.Capacity'. In order to do it, we will focus on both the $R^2$ and the MSE, trying to find the model that best maximizes the first one while minimasing the second one.

```{r}
Y <- log(AirbnbTrain$Price)

variables_to_add= list("Normalised.Attraction.Index", "Normalised.Restraunt.Index", "Bedrooms", "Person.Capacity")

#variables_to_add= list( "Bedrooms", "Person.Capacity", "Normalised.Attraction.Index", "Normalised.Restraunt.Index")

#variables_to_add= list("Bedrooms", "Normalised.Attraction.Index", "Normalised.Restraunt.Index",  "Person.Capacity")

#variables_to_add= list("Person.Capacity", "Normalised.Attraction.Index", "Normalised.Restraunt.Index", "Bedrooms" )

# Create an empty data frame to store models,R2 and RMSE
rmse_df <- data.frame(RMSE = numeric(length(variables_to_add)), R2= numeric(length(variables_to_add)), Variables = character(length(variables_to_add)), stringsAsFactors = FALSE)

models <- list()

# Fit models and calculate MSE
for (i in seq_along(variables_to_add)) {
  formula <- as.formula(paste("Y ~ ", paste(variables_to_add[1:i], collapse = " + ")))
  model <- lm(formula, data = AirbnbTrain)
  
  # Store the model
  models[[i]] <- model
  
  # Predict on the test set
  pred <- exp(predict(model, newdata = AirbnbTest))
  
  # Calculate MSE
  rmse <- sqrt(mean((pred - log(AirbnbTest$Price))^2))
  
  R2= cor(pred, log(AirbnbTest$Price))^2

  # Store MSE and variables in the data frame
  rmse_df[i, "RMSE"] <- rmse
  rmse_df[i, "R2"] <- R2
  rmse_df[i, "Variables"] <- paste(variables_to_add[1:i], collapse = ", ")
}

# Find the model with the lowest MSE
best_model_index <- which.min(rmse_df$RMSE)
best_model <- models[[best_model_index]]

# Print results
cat("Best model includes variables:", rmse_df$Variables[best_model_index], "\n")
cat("RMSE of the best model:", rmse_df$MSE[best_model_index], "\n")
cat("R2 of the best model:", rmse_df$R2[best_model_index], "\n")
rmse_df

```

After trying several combinations, we lead to the conclusion that, although adding the 'Bedrooms' and 'Person.Capacity' variables may increase the $R^2$, meaning is adding information, it also adds noise as the RMSE increases a lot. By the RMSE the best model is the one with only the first two variables (the ones more correlated), but we will choose to add also Bedrooms so $R^2$ is bigger enough.

Now, let's try to add interactions with the categorical variables. After trying several combinations, we lead to the conclusion that 'City' was the most informative variable in order to interact with the indexes. Later, we add 'Bedrooms' and tried several more interactions, however, any improvement was achieved. Finally, the best model we could fit was the following.

```{r}
best_linFit <- lm(log(Price) ~ City*(Normalised.Attraction.Index + Normalised.Restraunt.Index)^2 + sqrt(Bedrooms) , data=AirbnbTrain)

# Predict on the test set
pred_best <- exp(predict(best_linFit, newdata = AirbnbTest))
  
  # Calculate MSE
RMSE_best <- sqrt(mean((pred_best - log(AirbnbTest$Price))^2))
R2_best= cor(pred, log(AirbnbTest$Price))^2
R2_best
RMSE_best
```

As we have said before, this is a difficult application, as many of the variables do not add enough information. After trying many combinations, we have lead to a model with $R^2=0.2$ and $RMSE = 272$. Having in mind previous results, we have not been able to improve the benchmark, but we have been able to reduce the noise from the model with all the variables while maintaining the information gain.

For better conclusions we can perform some diagnosis. Contrary to the simple model, this one has achieved to be more homocedastic, having constant variance of the residuals and better distribution in general.

```{r}
plot(best_linFit, pch=23 ,bg='orange',cex=2)
```

### Prediction Intervals

Finally, we would evaluate the predictions of our model with the prediction intervals.

```{r}
pred_best <- predict(best_linFit, newdata = AirbnbTest, interval='prediction')

y = AirbnbTest$Price
yhat = exp(pred_best)

# Assuming pred.log contains the lower and upper prediction intervals
AirbnbTest$Pred = exp(pred_best[, "fit"])
AirbnbTest$Real = y
AirbnbTest$Lower = exp(pred_best[, "lwr"])
AirbnbTest$Upper = exp(pred_best[, "upr"])

# Create a data frame for plotting
plot_data <- data.frame(Real = AirbnbTest$Real,
                        Pred = AirbnbTest$Pred,
                        Lower = AirbnbTest$Lower,
                        Upper = AirbnbTest$Upper)
# Prediction Interval Plot

y_limits <- c(0, 10005) 
ggplot(plot_data, aes(x = Real, y = Pred)) +
  geom_point() +
  geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, fill = "blue") +
  labs(title = "Scatter Plot of Real vs. Predicted",
       x = "Real",
       y = "Predicted")+
  coord_cartesian(ylim = y_limits)
```

As we can observe, most of the points lies inside the shadow of the prediction intervals. To better evaluate the exact quantity we can count them.

```{r}
# Counting the points outside the intervals
outside_interval_count <- sum(AirbnbTest$Real < AirbnbTest$Lower | AirbnbTest$Real > AirbnbTest$Upper)

# Calculating the coverage
total_points <- nrow(AirbnbTest)
coverage <- round(100-(outside_interval_count / total_points) * 100, digits=1)

# Printing the coverage
print(paste("Percentage of points inside the intervals:", coverage, "%"))
```

Finally, our model is covering 95.5 % of the points inside the prediction intervals. That is, although before we determined that our predictions were not that accurate due to bad results on RMSE, we can say that our model is useful, as we are capturing well the noise. In general, is more important to focus the performance of a model on the intervals that on exact predictions, as usually we are interested on knowing a close value of the price, but not interested on the exact one.

# Generalised Linear Models

## Data preparation

Continuing with the second dataset, we will focus on predicting the average salary offered on several job postings related to the position of Data Scientist. To do it, we will try to use a Generalised Linear Model, but first, we need to clean our dataset.

```{r}
Salary_data <- read.csv("Data_Analyst_Jobs.csv")
```

First, we split on training and test, and evaluate the type of the variables.

```{r}
set.seed(123)

# split between training and testing sets
spl = createDataPartition(Salary_data$Sector, p = 0.8, list = FALSE)  # 80% for training

Salary_data_Train = Salary_data[spl,]
Salary_data_Test = Salary_data[-spl,]

str(Salary_data_Train)
```

The objective variable would be 'Avg.Salary.K', that is an already cleaned variable from 'Salary.Estimate', measuring the mean of the salaries offer in thousands of dollars. Therefore, we could get rid of this last variable.

Apart from this one, we could observe several other variables that are already clean on another column, as 'Company.Name' and 'company_txt', 'Job.Title' and 'job_title_sim', and finally, 'Location' and 'Job.Location'.

Other variables we would eliminate are the 'Job.Description', as is a natural language column that would not gives as any valuable information. Similarly, index is not giving any information so would be also eliminated.

Finally, 'Lower.Salary' and 'Upper.Salary' would be erased as they can easily predict the objective variable by performing the mean of the two, so would be useless to develop a complete model if they alone can predict it.

```{r}
Salary_data_Train <- subset(Salary_data_Train, select = -c(Job.Description, Job.Title, Location, index, Salary.Estimate, Lower.Salary, Upper.Salary, Company.Name ))

```

### Null values and categorical variables

With the remaining features, we need to evaluate the null values. Although it can seem there is none, is because they are set as 'na' or '-1'. Columns with value -1 means either the data scraping was unsuccessful for that or the data was not present. Therefore, we would evaluate the classes of the features to observe if the null values comprehend a significant quantity.

```{r}
any(is.na(Salary_data_Train))
```

We can observe some null values on these first set of features, but not as much to be considered significant. However, we observe the variable 'Headquarters' has too many unique categories, which would not be helpful for developing the model, therefore we decided to remove it.

```{r}
#We comment the vairables where there are many categories to avoid a long notebook
#table(Salary_data_Train$Headquarters)
table(Salary_data_Train$Size)
table(Salary_data_Train$Type.of.ownership)
#table(Salary_data_Train$Industry )
table(Salary_data_Train$Sector )

Salary_data_Train <- subset(Salary_data_Train, select = -Headquarters)

```

The following variables 'Industry' and 'Sector' represents the same term, so we would only keep the one with less unique values.

```{r}
length(unique(Salary_data_Train$Industry))
length(unique(Salary_data_Train$Sector))

```

We keep the 'Sector' one.

```{r}
Salary_data_Train <- subset(Salary_data_Train, select = -Industry)

```

Finishing with the null, 'Competitors' would be erased due to having 372 null rows of 595. 'Revenue' has 156 of 595, is almost 30% of the rows, however, we will try to keep them, as the remaining ones could have significant information. 'company_txt' would also be removed due to having too many unique values.

```{r}
table(Salary_data_Train$Revenue)
#table(Salary_data_Train$Competitors)
#table(Salary_data_Train$company_txt )

```

```{r}
Salary_data_Train <- subset(Salary_data_Train, select = -c(Competitors, company_txt))


```

For the same reason, 'seniority_by_title' and 'Degree' are also removed.

```{r}
table(Salary_data_Train$Job.Location)
table(Salary_data_Train$job_title_sim)
table(Salary_data_Train$seniority_by_title)
table(Salary_data_Train$Degree)
```

```{r}
Salary_data_Train <- subset(Salary_data_Train, select = -c(seniority_by_title,Degree))

```

### Binary variables

This dataset, apart from many categorical variables, has also many binary ones. Most of them refer to skills required or not for the job offer, and others represent if salary was reported in hourly rate or if salary was provided by the employer. Let's observe how they look to detect some unbalanced classes or not.

```{r}
table(Salary_data_Train$Hourly) # 1: If salary was reported in hourly rate. 0: Otherwise
table(Salary_data_Train$Employer.provided) # 1: If the salary was provided by the employer 0: Otherwise

# 1: If x skill is required 0: Otherwise

table(Salary_data_Train$Python)
table(Salary_data_Train$spark)
table(Salary_data_Train$aws)
table(Salary_data_Train$excel)
table(Salary_data_Train$sql)
table(Salary_data_Train$sas)
table(Salary_data_Train$keras)
table(Salary_data_Train$pytorch)
table(Salary_data_Train$scikit)
table(Salary_data_Train$tensor)
table(Salary_data_Train$hadoop)
table(Salary_data_Train$tableau)
table(Salary_data_Train$bi)
table(Salary_data_Train$flink)
table(Salary_data_Train$mongo)
table(Salary_data_Train$google_an) #1: If Google analytics certificate is required 0: Otherwise


```

Most of them are kind of unbalanced, but they may be helpful for the matter of hand. So, as there is no null values, we would keep all of them.

### Numeric variables

This dataset has few numeric variables, naming, 'Avg.Salary.K' (the objective one), 'Founded' (year of foundation of the company offering the job), 'Age' (of the company) and 'Rating' (of the company). These last two are redundant, as one can be obtained from the other one, so we only would keep the age.

```{r}
Salary_data_Train <- subset(Salary_data_Train, select = -Founded)

```

We first observe how many null values (-1) there are. As is not significant, we keep all.

```{r}
table(Salary_data_Train$Rating) # It gives the rating of the company
#table(Salary_data_Train$Age) # Age of the company (in yrs)
#table(Salary_data_Train$Avg.Salary.K.)

```

Next, we evaluate some metrics to better understand all of them.

```{r}
summary(Salary_data_Train$Rating) # It gives the rating of the company
summary(Salary_data_Train$Age) # Age of the company (in yrs)
summary(Salary_data_Train$Avg.Salary.K.)
```

### Final dataset

With all of the preprocessing, the training data finally looks like this.

```{r}
Salary_data_Train
```

## Explanatory Analysis

During this explanatory analysis, we would better focus on analyzing the distribution of our objective variable and the possible relationships of the other with it. As previously explained, our objective variable is numeric representing the mean salary per year on thousands of dollars. It is important to know the distribution before developing a GLM, so the model can choose the appropriate link function. Therefore, we would try to fit three different distributions and we would choose the best approach.

```{r}
lambda <- mean(Salary_data_Train$Avg.Salary.K.)
mean_val <- mean(Salary_data_Train$Avg.Salary.K.)
sd_val <- sd(Salary_data_Train$Avg.Salary.K.)

rate  = mean_val/var(Salary_data_Train$Avg.Salary.K.)
shape = rate * mean_val

xgrid = seq(0,250,0.1)

ggplot(Salary_data_Train, aes(Avg.Salary.K.)) +
  geom_density(fill = "lightblue") +
  geom_line(data = data.frame(x = 0:250, y = dpois(0:250, lambda = lambda)), aes(x = x, y = y), color = "red", linewidth = 1) + 
  geom_line(data = data.frame(x = xgrid, y = dnorm(xgrid, mean = mean_val, sd = sd_val)),
            aes(x = x, y = y), color = "blue", linewidth = 1) +
  geom_line(data = data.frame(x = xgrid, y = dgamma(xgrid, shape = shape, rate = rate)),
            aes(x = x, y = y), color = "green", linewidth = 1) +
  ggtitle("Avg Salary distribution")

```

As can be observed, the Gamma distribution is the one that better fits our dataset, therefore, we would develop the GLM using it.

Furthermore, we can plot the objective variable in terms of other ones to find relations. For example, here we observe how the salary distribution differs a lot from one type of job offer from another.

```{r}
ggplot(Salary_data_Train, aes(Avg.Salary.K.)) + geom_density(aes(group=job_title_sim, colour=job_title_sim, fill=job_title_sim), alpha=0.1) + ggtitle("Salary distribution by job title")
```

If we perform the box plot of the previous relationship, we better visualize how the offers for the 'director' position are the ones with biggest mean salary, or how the 'data scientist' offers are the ones with more variance of salaries.

```{r}
ggplot(Salary_data_Train, aes(x = job_title_sim, y = Avg.Salary.K.)) +
  geom_boxplot() +
  labs(title = "Boxplot between Avg Salary and Job Title",
       x = "Job title",
       y = "Avg.Salary.K.")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Finally, we try to find some trend related with the rating of the companies and the mean salary, however, the scatter plot does not show any useful information.

```{r}
ggplot(Salary_data_Train, aes(x = Avg.Salary.K., y = Rating)) +
  geom_point() +
  labs(title = "Scatter Plot entre Avg.Salary.K. y Rating",
       x = "Avg.Salary.K.",
       y = "Rating")
```

### Correlations

Before starting modeling the predictor, we try to find the numeric (and binary) variables more correlated with the objective feature.

```{r}
# Calcular correlaciones
correlations <- cor(Salary_data_Train[, c("Age", "Rating", "Hourly", "Avg.Salary.K.", "Employer.provided", "Python", "spark", "aws", "excel", "sql", "sas", "keras", "pytorch", "scikit", "tensor", "hadoop", "tableau", "bi", "flink", "mongo", "google_an")])

# Extraer las correlaciones con la variable objetivo "Avg.Salary.K."
corr <- sort(correlations["Avg.Salary.K.",], decreasing = F)

# Crear un dataframe con las correlaciones
corr_df <- data.frame(variable = names(corr), correlation = corr)

# Crear el gráfico de barras
ggplot(corr_df, aes(x = variable, y = correlation)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(x = "", y = "Correlation", title = "Correlations with Avg.Salary.K.") +
  theme(plot.title = element_text(hjust = 0.5, size = rel(1.5)), axis.text.x = element_text(angle = 45, hjust = 1))
```

As with the previous dataset, this application would not be easy as most of the variables does not have any correlation with the objective one. However, we can highlight the values of 'scikit', 'Hourly' and 'Python'.

## Model development and interpretation

### Benchmark

Following the same criteria as with the previous application, in order to develop our model, we must first set the benchmark from where we would try to improve the prediction RMSE, or at least, get as close as possible.

```{r}
bench_glm = glm(Avg.Salary.K. ~ 1 , family = Gamma, data = Salary_data_Train) 
#summary(bench_glm)

pred_bench_glm= predict(bench_glm, newdata= Salary_data_Test)

#Let's know validate the predictions, calculating R^2 in the testing set

# Evaluate performance on the test set
RMSE_test <- sqrt(mean((pred_bench_glm - Salary_data_Test$Avg.Salary.K.)^2))
cat("RMSE on test set:", RMSE_test, "\n")
```

### Developing the model

We would make a first approach, in order to evaluate the marginal effects of each variable into the model. Later, we would try to choose the best variables to define the final model. As we can observed, with this complex model we have reached $R^2= 0.39$ and $RMSE= 110.95$, so, by using all the variables, we have many information but also more noise, so RMSE is similar to benchmark.

```{r}
## First glm
first_glm = glm(Avg.Salary.K. ~ job_title_sim+ Rating + Hourly + Employer.provided + Python + spark + aws + excel + sql + sas + keras + pytorch + scikit + tensor + hadoop + tableau + bi + flink + mongo + google_an + Size + Type.of.ownership + Sector + Revenue+ Job.Location + Age, family = Gamma, data = Salary_data_Train) 
#summary(first_glm)

pred_f_glm= predict(first_glm, newdata= Salary_data_Test)

#Let's know validate the predictions, calculating R^2 in the testing set

# Evaluate performance on the test set
R2_test <- cor(pred_f_glm, Salary_data_Test$Avg.Salary.K.)^2
RMSE_test <- sqrt(mean((pred_f_glm - Salary_data_Test$Avg.Salary.K.)^2))

cat("R-squared on test set:", R2_test, "\n")
cat("RMSE on test set:", RMSE_test, "\n")
```

```{r}
library(effects)
variables = list("job_title_sim", "Rating", "Hourly", "Employer.provided" ,"Python" ,"spark", "aws" ,"excel", "sql" , "sas" , "keras" , "pytorch" , "scikit" ,"tensor" ,"hadoop" , "tableau" , "bi" , "flink" , "mongo" , "google_an", "Size", "Type.of.ownership",  "Sector",  "Revenue",  "Job.Location" , "Age")

#job_title_sim
plot(effect(variables[[1]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")


#Hourly
plot(effect(variables[[3]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="") 

#Python
plot(effect(variables[[5]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="") 

#sql
plot(effect(variables[[9]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="") 

#Size
plot(effect(variables[[21]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="") 

#Type.of.ownership
plot(effect(variables[[22]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="") 

#Sector
plot(effect(variables[[23]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="") 

#Revenue
plot(effect(variables[[24]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="") 

#Job.Location
plot(effect(variables[[25]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="") 

```

As we can observe, after analyzing the individual effects of the variables on the model, we can get better inshigths. For example, variables as 'job_title_sim', 'Hourly', 'Python' or 'Revenue' gives many of the information to the model, as there is a clear difference between the values and the objective function and the intervals are well defined. However, other such as 'sql' or 'Job.Location' are not useful for the model, because there is a lot of uncertainty on the intervals and the mean salary value remains constant for all the x axis, so they are only adding noise.

We develop another model with the most informative variables (still without interactions). This model has lower $R^2$ value, so we will try on our final model add the necessary interactions to increase it by using only the necessary variables.

```{r}
## First glm
other_glm = glm(Avg.Salary.K. ~ job_title_sim + Hourly + Employer.provided   + google_an + Size + Type.of.ownership + Sector + Python + Revenue + sas, family = Gamma, data = Salary_data_Train) 
#summary(first_glm)

pred_other_glm= predict(other_glm, newdata= Salary_data_Test)

#Let's know validate the predictions, calculating R^2 in the testing set

# Evaluate performance on the test set
R2_test <- cor(pred_other_glm, Salary_data_Test$Avg.Salary.K.)^2
RMSE_test <- sqrt(mean((pred_other_glm - Salary_data_Test$Avg.Salary.K.)^2))

cat("R-squared on test set:", R2_test, "\n")
cat("RMSE on test set:", RMSE_test, "\n")
```

### Final model

After trying several combinations, we have reached to an $R^2 = 0.31$, close to the $R^2 = 0.39$ of the more complex model, but only using the most informative variables and the interactions between them. Let's analyse the effects of them in the final model.

```{r}
## First glm
final_glm = glm(Avg.Salary.K. ~ job_title_sim*Python  + Sector + Revenue*sas , family = Gamma, data = Salary_data_Train) 
#summary(first_glm)

pred_final_glm= predict(final_glm, newdata= Salary_data_Test)

#Let's know validate the predictions, calculating R^2 in the testing set

# Evaluate performance on the test set
R2_test <- cor(pred_final_glm, Salary_data_Test$Avg.Salary.K.)^2
RMSE_test <- sqrt(mean((pred_final_glm - Salary_data_Test$Avg.Salary.K.)^2))

cat("R-squared on test set:", R2_test, "\n")
cat("RMSE on test set:", RMSE_test, "\n")
```

#### Effects

By using another library that supports the interactions of the model, we can observe the effects of each element on the GLM. Starting with the marginal component, 'Sector' is useful mainly for one category. Later, the interactions between 'Python' and 'job_title_sim' are much informative, having in mind the scale of the y axis. Also, 'Revenue' and 'sas' gives the objective variable much of the variance. All in all, this is a simpler model that from we started (then, it would not tend to overfit), but having most of the information of the application.

```{r}
library(sjPlot)
#type pred: Predicted values (marginal effects) for specific model terms
#type int: Marginal effects of interaction terms in model
plot_model(final_glm, type = "pred", terms='Sector')+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot_model(final_glm, type = "int")[[1]]+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot_model(final_glm, type = "int")[[2]]+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### Predictive performance: Prediction Intervals

We have already evaluate the $R^2$ and $RMSE$ values, let's know plot the prediction intervals of the final model over the test set. We would follow two different approaches:

-   Firstly, we would directly use the standard errors provided by the predict function for the gamma GLM model.
-   Secondly, we would calculate the prediction intervals by transforming the bounds on the linear scale using the inverse link function of the gamma distribution.

```{r}
##First approach
pred_final_glm= predict(final_glm, newdata= Salary_data_Test, type = "response", se.fit = TRUE)

# Create a data frame for plotting
plot_data <- data.frame(Real = Salary_data_Test$Avg.Salary.K.,
                        Pred =  pred_final_glm$fit,
                        Lower = pred_final_glm$fit - 1.96 * pred_final_glm$se.fit,
                        Upper = pred_final_glm$fit + 1.96 * pred_final_glm$se.fit)
# Prediction Interval Plot
ggplot(plot_data, aes(x = Real, y = Pred)) +
  geom_point() +
  geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, fill = "blue") +
  labs(title = "Scatter Plot of Real vs. Predicted",
       x = "Real",
       y = "Predicted")
```

As we can observe, most of the points lies inside the shadow of the prediction intervals. To better evaluate the exact quantity we can count them.

```{r}
# Counting the points outside the intervals
outside_interval_count <- sum(Salary_data_Test$Avg.Salary.K. < pred_final_glm$fit - 1.96 * pred_final_glm$se.fit | Salary_data_Test$Avg.Salary.K. > pred_final_glm$fit + 1.96 * pred_final_glm$se.fit)

# Calculating the coverage
total_points <- nrow(Salary_data_Test)
coverage <- round(100-(outside_interval_count / total_points) * 100, digits=1)

# Printing the coverage
print(paste("Percentage of points inside the intervals approach 1 :", coverage, "%"))
```

Althougt on the plot we could visualize good predictions, when counting the points inside the intervals we get less that half of them. Let's now evaluate the second approach.

```{r}
## Second approach

# Extract necessary information
fit <- pred_final_glm$fit
se_fit <- pred_final_glm$se.fit

# Calculate prediction intervals using the inverse link function and Gamma distribution properties
lower_limit <- fit * exp(-1.96 * se_fit)
upper_limit <- fit * exp(1.96 * se_fit)

# Create a data frame for plotting
plot_data <- data.frame(
  Real = Salary_data_Test$Avg.Salary.K.,
  Pred = fit,
  Lower = lower_limit,
  Upper = upper_limit
)

y_limits <- c(0, 10^18) 
ggplot(plot_data, aes(x = Real, y = Pred)) +
  geom_point() +
  geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, fill = "blue") +
  labs(title = "Scatter Plot of Real vs. Predicted",
       x = "Real",
       y = "Predicted")+  coord_cartesian(ylim = y_limits)
```

With this approach is much more difficult to visualize in a plot the prediction intervals, as the y axis limits are much larger, however, when evaluating the points inside the intervals, we observe how the predictions works perfectly.

```{r}
# Counting the points inside the intervals
inside_interval_count <- sum(Salary_data_Test$Avg.Salary.K. > lower_limit & Salary_data_Test$Avg.Salary.K. < upper_limit)

# Calculating the coverage
total_points <- nrow(Salary_data_Test)
coverage <- round((inside_interval_count / total_points) * 100, digits = 1)

# Printing the coverage
print(paste("Percentage of points inside the intervals approach 2:", coverage, "%"))

```

For the sake of interpretation, the first approach is better, however, second approach gives us better prediction results. In general, we could say that our final model has achieved to predict at least as well as the benchmark, as the RMSE are equal, and has achieved to predict inside the prediction intervals in most of the cases.

# Conclusions

In conclusion, this notebook outlines two distinct approaches to modeling a predictive application. In the context of the selected datasets, linear regression presents challenges in enhancing the benchmark, yet the final model successfully captures the majority of data points within the prediction intervals.

Conversely, Generalized Linear Models prove to be more adept at fitting the data distribution, yielding results comparable to the benchmark and encompassing the majority of data points within the prediction intervals.

In both scenarios, we emphasize the significance of interactions and variable selection. These aspects play a crucial role in developing a model that maximizes information while minimizing the impact of noise, thereby contributing to a more robust and reliable predictive framework.
