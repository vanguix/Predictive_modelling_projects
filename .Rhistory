#Python
plot(effect(variables[[5]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#sql
plot(effect(variables[[9]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Size
plot(effect(variables[[21]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Type.of.ownership
plot(effect(variables[[22]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Sector
plot(effect(variables[[23]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Revenue
plot(effect(variables[[24]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Job.Location
plot(effect(variables[[25]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
library(tidyverse)
library(MASS)
library(caret) #ML tools
library(e1071)
library(ggplot2)
par(mar = c(2, 2, 2, 2))
# Loading and preparing data
Airbnb <- read.csv("Airbnb_data.csv")
Airbnb
set.seed(123)
# split between training and testing sets
spl = createDataPartition(Airbnb$Price, p = 0.8, list = FALSE)  # 80% for training
AirbnbTrain = Airbnb[spl,]
AirbnbTest = Airbnb[-spl,]
str(AirbnbTrain)
any(is.na(AirbnbTrain))
summary(AirbnbTrain)
boxplot(AirbnbTrain$Price)
AirbnbTrain <- subset(AirbnbTrain, select = -c(Attraction.Index, Restraunt.Index ))
table(AirbnbTrain$City)
table(AirbnbTrain$Day)
table(AirbnbTrain$Room.Type)
table(AirbnbTrain$Shared.Room)
table(AirbnbTrain$Private.Room)
table(AirbnbTrain$Superhost)
table(AirbnbTrain$Multiple.Rooms)
table(AirbnbTrain$Business)
AirbnbTrain <- subset(AirbnbTrain, select = -c(Private.Room, Shared.Room ))
AirbnbTrain$Superhost<-as.numeric(as.logical(AirbnbTrain$Superhost))
ggplot(AirbnbTrain, aes(Price)) + geom_density(fill="lightblue") + xlab("Price") + ggtitle("Price distribution")
## from the statistics obtained before we know price has no 0 values as minimum is 34, so we can apply the logarithm to get a better distribution
ggplot(AirbnbTrain, aes(log(Price))) + geom_density(fill="lightblue") + xlab("log(Price)") + ggtitle("Price distribution")
mean(log(AirbnbTrain$Price))
exp(mean(log(AirbnbTrain$Price)))
ggplot(AirbnbTrain, aes(log(Price))) + geom_density(aes(group=City, colour=City, fill=City), alpha=0.1) + ggtitle("Price distribution")
ggplot(AirbnbTrain, aes(x=City.Center..km., y=log(Price), color=City)) + geom_point(alpha=0.8) + ggtitle("Price vs City Center distance")
ggplot(AirbnbTrain, aes(x=Metro.Distance..km., y=log(Price), color=City)) + geom_point(alpha=0.8) + ggtitle("Price vs Metro distance")
ggplot(AirbnbTrain, aes(x = Bedrooms, y = log(Price), fill = City)) +
geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
ggtitle("Price vs Number of bedrooms")
table(AirbnbTrain$Bedrooms)
ggplot(AirbnbTrain, aes(x=Normalised.Attraction.Index, y=log(Price), color=City)) + geom_point(alpha=0.8) + ggtitle("Price vs Attraction Index")
ggplot(AirbnbTrain, aes(x=Normalised.Restraunt.Index, y=log(Price), color=City)) + geom_point(alpha=0.8) + ggtitle("Price vs Restraunt Index")
## Moving the objective function to the last position
AirbnbTrain<-AirbnbTrain[, c(1:(2 - 1), (2 + 1):ncol(AirbnbTrain), 2)]
corr_delay <- sort(cor(AirbnbTrain[,c(4:15)])["Price",], decreasing = T)
corr=data.frame(corr_delay)
ggplot(corr,aes(x = row.names(corr), y = corr_delay)) +
geom_bar(stat = "identity", fill = "lightblue") +
scale_x_discrete(limits= row.names(corr)) +
labs(x = "", y = "Price", title = "Correlations") +
theme(plot.title = element_text(hjust = 0, size = rel(1.5)),
axis.text.x = element_text(angle = 45, hjust = 1))
simple_linFit <- lm(log(Price) ~ Normalised.Attraction.Index, data=AirbnbTrain)
summary(simple_linFit)
# Scatter plot
ggplot(AirbnbTrain, aes(x = Normalised.Attraction.Index, y = log(Price))) +
geom_point(alpha = 0.8, color= 'lightblue') +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
ggtitle("Price vs Attraction Index regression")
pred_simple= exp(predict(simple_linFit, newdata= AirbnbTest))
#Let's know validate the predictions, calculating R^2 in the testing set
R2_simple= cor(pred_simple, log(AirbnbTest$Price))^2
R2_simple
par(mar = c(2, 2, 2, 2))
plot(simple_linFit, pch=23 ,bg='orange',cex=2)
benchFit <- lm(log(Price) ~ 1, data=AirbnbTrain)
pred_bench <- exp(predict(benchFit, newdata=AirbnbTest))
#MSE comparison: my model vs benchmark
RMSE_simple=sqrt(mean((pred_simple - log(AirbnbTest$Price))^2))
RMSE_bench= sqrt(mean((pred_bench - log(AirbnbTest$Price))^2))
RMSE_simple
RMSE_bench
AirbnbTest$Superhost<-as.numeric(as.logical(AirbnbTest$Superhost))
all_linFit <- lm(log(Price) ~ ., data=AirbnbTrain)
# Predict on the test set
pred_all <- exp(predict(all_linFit, newdata = AirbnbTest))
# Calculate MSE
RMSE_all <- sqrt(mean((pred_all - AirbnbTest$Price)^2))
R2_all= cor(pred_all, AirbnbTest$Price)^2
summary(all_linFit)
R2_all
RMSE_all
Y <- log(AirbnbTrain$Price)
variables_to_add= list("Normalised.Attraction.Index", "Normalised.Restraunt.Index", "Bedrooms", "Person.Capacity")
#variables_to_add= list( "Bedrooms", "Person.Capacity", "Normalised.Attraction.Index", "Normalised.Restraunt.Index")
#variables_to_add= list("Bedrooms", "Normalised.Attraction.Index", "Normalised.Restraunt.Index",  "Person.Capacity")
#variables_to_add= list("Person.Capacity", "Normalised.Attraction.Index", "Normalised.Restraunt.Index", "Bedrooms" )
# Create an empty data frame to store models,R2 and RMSE
rmse_df <- data.frame(RMSE = numeric(length(variables_to_add)), R2= numeric(length(variables_to_add)), Variables = character(length(variables_to_add)), stringsAsFactors = FALSE)
models <- list()
# Fit models and calculate MSE
for (i in seq_along(variables_to_add)) {
formula <- as.formula(paste("Y ~ ", paste(variables_to_add[1:i], collapse = " + ")))
model <- lm(formula, data = AirbnbTrain)
# Store the model
models[[i]] <- model
# Predict on the test set
pred <- exp(predict(model, newdata = AirbnbTest))
# Calculate MSE
rmse <- sqrt(mean((pred - log(AirbnbTest$Price))^2))
R2= cor(pred, log(AirbnbTest$Price))^2
# Store MSE and variables in the data frame
rmse_df[i, "RMSE"] <- rmse
rmse_df[i, "R2"] <- R2
rmse_df[i, "Variables"] <- paste(variables_to_add[1:i], collapse = ", ")
}
# Find the model with the lowest MSE
best_model_index <- which.min(rmse_df$MSE)
best_model <- models[[best_model_index]]
Y <- log(AirbnbTrain$Price)
variables_to_add= list("Normalised.Attraction.Index", "Normalised.Restraunt.Index", "Bedrooms", "Person.Capacity")
#variables_to_add= list( "Bedrooms", "Person.Capacity", "Normalised.Attraction.Index", "Normalised.Restraunt.Index")
#variables_to_add= list("Bedrooms", "Normalised.Attraction.Index", "Normalised.Restraunt.Index",  "Person.Capacity")
#variables_to_add= list("Person.Capacity", "Normalised.Attraction.Index", "Normalised.Restraunt.Index", "Bedrooms" )
# Create an empty data frame to store models,R2 and RMSE
rmse_df <- data.frame(RMSE = numeric(length(variables_to_add)), R2= numeric(length(variables_to_add)), Variables = character(length(variables_to_add)), stringsAsFactors = FALSE)
models <- list()
# Fit models and calculate MSE
for (i in seq_along(variables_to_add)) {
formula <- as.formula(paste("Y ~ ", paste(variables_to_add[1:i], collapse = " + ")))
model <- lm(formula, data = AirbnbTrain)
# Store the model
models[[i]] <- model
# Predict on the test set
pred <- exp(predict(model, newdata = AirbnbTest))
# Calculate MSE
rmse <- sqrt(mean((pred - log(AirbnbTest$Price))^2))
R2= cor(pred, log(AirbnbTest$Price))^2
# Store MSE and variables in the data frame
rmse_df[i, "RMSE"] <- rmse
rmse_df[i, "R2"] <- R2
rmse_df[i, "Variables"] <- paste(variables_to_add[1:i], collapse = ", ")
}
# Find the model with the lowest MSE
best_model_index <- which.min(rmse_df$RMSE)
best_model <- models[[best_model_index]]
# Print results
cat("Best model includes variables:", rmse_df$Variables[best_model_index], "\n")
cat("RMSE of the best model:", rmse_df$MSE[best_model_index], "\n")
cat("R2 of the best model:", rmse_df$R2[best_model_index], "\n")
rmse_df
best_linFit <- lm(log(Price) ~ City*(Normalised.Attraction.Index + Normalised.Restraunt.Index)^2 + sqrt(Bedrooms) , data=AirbnbTrain)
# Predict on the test set
pred_best <- exp(predict(best_linFit, newdata = AirbnbTest))
# Calculate MSE
RMSE_best <- sqrt(mean((pred_best - log(AirbnbTest$Price))^2))
R2_best= cor(pred, log(AirbnbTest$Price))^2
R2_best
RMSE_best
plot(best_linFit, pch=23 ,bg='orange',cex=2)
pred_best <- predict(best_linFit, newdata = AirbnbTest, interval='prediction')
y = AirbnbTest$Price
yhat = exp(pred_best)
# Assuming pred.log contains the lower and upper prediction intervals
AirbnbTest$Pred = exp(pred_best[, "fit"])
AirbnbTest$Real = y
AirbnbTest$Lower = exp(pred_best[, "lwr"])
AirbnbTest$Upper = exp(pred_best[, "upr"])
# Create a data frame for plotting
plot_data <- data.frame(Real = AirbnbTest$Real,
Pred = AirbnbTest$Pred,
Lower = AirbnbTest$Lower,
Upper = AirbnbTest$Upper)
# Prediction Interval Plot
y_limits <- c(0, 10005)
ggplot(plot_data, aes(x = Real, y = Pred)) +
geom_point() +
geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, fill = "blue") +
labs(title = "Scatter Plot of Real vs. Predicted",
x = "Real",
y = "Predicted")+
coord_cartesian(ylim = y_limits)
# Counting the points outside the intervals
outside_interval_count <- sum(AirbnbTest$Real < AirbnbTest$Lower | AirbnbTest$Real > AirbnbTest$Upper)
# Calculating the coverage
total_points <- nrow(AirbnbTest)
coverage <- round(100-(outside_interval_count / total_points) * 100, digits=1)
# Printing the coverage
print(paste("Percentage of points inside the intervals:", coverage, "%"))
Salary_data <- read.csv("Data_Analyst_Jobs.csv")
set.seed(123)
# split between training and testing sets
spl = createDataPartition(Salary_data$Sector, p = 0.8, list = FALSE)  # 80% for training
Salary_data_Train = Salary_data[spl,]
Salary_data_Test = Salary_data[-spl,]
str(Salary_data_Train)
Salary_data_Train <- subset(Salary_data_Train, select = -c(Job.Description, Job.Title, Location, index, Salary.Estimate, Lower.Salary, Upper.Salary, Company.Name ))
any(is.na(Salary_data_Train))
#We comment the vairables where there are many categories to avoid a long notebook
#table(Salary_data_Train$Headquarters)
table(Salary_data_Train$Size)
table(Salary_data_Train$Type.of.ownership)
#table(Salary_data_Train$Industry )
table(Salary_data_Train$Sector )
Salary_data_Train <- subset(Salary_data_Train, select = -Headquarters)
length(unique(Salary_data_Train$Industry))
length(unique(Salary_data_Train$Sector))
Salary_data_Train <- subset(Salary_data_Train, select = -Industry)
table(Salary_data_Train$Revenue)
#table(Salary_data_Train$Competitors)
#table(Salary_data_Train$company_txt )
Salary_data_Train <- subset(Salary_data_Train, select = -c(Competitors, company_txt))
table(Salary_data_Train$Job.Location)
table(Salary_data_Train$job_title_sim)
table(Salary_data_Train$seniority_by_title)
table(Salary_data_Train$Degree)
Salary_data_Train <- subset(Salary_data_Train, select = -c(seniority_by_title,Degree))
table(Salary_data_Train$Hourly) # 1: If salary was reported in hourly rate. 0: Otherwise
table(Salary_data_Train$Employer.provided) # 1: If the salary was provided by the employer 0: Otherwise
# 1: If x skill is required 0: Otherwise
table(Salary_data_Train$Python)
table(Salary_data_Train$spark)
table(Salary_data_Train$aws)
table(Salary_data_Train$excel)
table(Salary_data_Train$sql)
table(Salary_data_Train$sas)
table(Salary_data_Train$keras)
table(Salary_data_Train$pytorch)
table(Salary_data_Train$scikit)
table(Salary_data_Train$tensor)
table(Salary_data_Train$hadoop)
table(Salary_data_Train$tableau)
table(Salary_data_Train$bi)
table(Salary_data_Train$flink)
table(Salary_data_Train$mongo)
table(Salary_data_Train$google_an) #1: If Google analytics certificate is required 0: Otherwise
Salary_data_Train <- subset(Salary_data_Train, select = -Founded)
table(Salary_data_Train$Rating) # It gives the rating of the company
#table(Salary_data_Train$Age) # Age of the company (in yrs)
#table(Salary_data_Train$Avg.Salary.K.)
summary(Salary_data_Train$Rating) # It gives the rating of the company
summary(Salary_data_Train$Age) # Age of the company (in yrs)
summary(Salary_data_Train$Avg.Salary.K.)
Salary_data_Train
lambda <- mean(Salary_data_Train$Avg.Salary.K.)
mean_val <- mean(Salary_data_Train$Avg.Salary.K.)
sd_val <- sd(Salary_data_Train$Avg.Salary.K.)
rate  = mean_val/var(Salary_data_Train$Avg.Salary.K.)
shape = rate * mean_val
xgrid = seq(0,250,0.1)
ggplot(Salary_data_Train, aes(Avg.Salary.K.)) +
geom_density(fill = "lightblue") +
geom_line(data = data.frame(x = 0:250, y = dpois(0:250, lambda = lambda)), aes(x = x, y = y), color = "red", linewidth = 1) +
geom_line(data = data.frame(x = xgrid, y = dnorm(xgrid, mean = mean_val, sd = sd_val)),
aes(x = x, y = y), color = "blue", linewidth = 1) +
geom_line(data = data.frame(x = xgrid, y = dgamma(xgrid, shape = shape, rate = rate)),
aes(x = x, y = y), color = "green", linewidth = 1) +
ggtitle("Avg Salary distribution")
ggplot(Salary_data_Train, aes(Avg.Salary.K.)) + geom_density(aes(group=job_title_sim, colour=job_title_sim, fill=job_title_sim), alpha=0.1) + ggtitle("Salary distribution by job title")
ggplot(Salary_data_Train, aes(x = job_title_sim, y = Avg.Salary.K.)) +
geom_boxplot() +
labs(title = "Boxplot between Avg Salary and Job Title",
x = "Job title",
y = "Avg.Salary.K.")+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(Salary_data_Train, aes(x = Avg.Salary.K., y = Rating)) +
geom_point() +
labs(title = "Scatter Plot entre Avg.Salary.K. y Rating",
x = "Avg.Salary.K.",
y = "Rating")
# Calcular correlaciones
correlations <- cor(Salary_data_Train[, c("Age", "Rating", "Hourly", "Avg.Salary.K.", "Employer.provided", "Python", "spark", "aws", "excel", "sql", "sas", "keras", "pytorch", "scikit", "tensor", "hadoop", "tableau", "bi", "flink", "mongo", "google_an")])
# Extraer las correlaciones con la variable objetivo "Avg.Salary.K."
corr <- sort(correlations["Avg.Salary.K.",], decreasing = F)
# Crear un dataframe con las correlaciones
corr_df <- data.frame(variable = names(corr), correlation = corr)
# Crear el gráfico de barras
ggplot(corr_df, aes(x = variable, y = correlation)) +
geom_bar(stat = "identity", fill = "lightblue") +
labs(x = "", y = "Correlation", title = "Correlations with Avg.Salary.K.") +
theme(plot.title = element_text(hjust = 0.5, size = rel(1.5)), axis.text.x = element_text(angle = 45, hjust = 1))
bench_glm = glm(Avg.Salary.K. ~ 1 , family = Gamma, data = Salary_data_Train)
#summary(bench_glm)
pred_bench_glm= predict(bench_glm, newdata= Salary_data_Test)
#Let's know validate the predictions, calculating R^2 in the testing set
# Evaluate performance on the test set
RMSE_test <- sqrt(mean((pred_bench_glm - Salary_data_Test$Avg.Salary.K.)^2))
cat("RMSE on test set:", RMSE_test, "\n")
## First glm
first_glm = glm(Avg.Salary.K. ~ job_title_sim+ Rating + Hourly + Employer.provided + Python + spark + aws + excel + sql + sas + keras + pytorch + scikit + tensor + hadoop + tableau + bi + flink + mongo + google_an + Size + Type.of.ownership + Sector + Revenue+ Job.Location + Age, family = Gamma, data = Salary_data_Train)
#summary(first_glm)
pred_f_glm= predict(first_glm, newdata= Salary_data_Test)
#Let's know validate the predictions, calculating R^2 in the testing set
# Evaluate performance on the test set
R2_test <- cor(pred_f_glm, Salary_data_Test$Avg.Salary.K.)^2
RMSE_test <- sqrt(mean((pred_f_glm - Salary_data_Test$Avg.Salary.K.)^2))
cat("R-squared on test set:", R2_test, "\n")
cat("RMSE on test set:", RMSE_test, "\n")
library(effects)
variables = list("job_title_sim", "Rating", "Hourly", "Employer.provided" ,"Python" ,"spark", "aws" ,"excel", "sql" , "sas" , "keras" , "pytorch" , "scikit" ,"tensor" ,"hadoop" , "tableau" , "bi" , "flink" , "mongo" , "google_an", "Size", "Type.of.ownership",  "Sector",  "Revenue",  "Job.Location" , "Age")
#job_title_sim
plot(effect(variables[[1]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Hourly
plot(effect(variables[[3]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Python
plot(effect(variables[[5]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#sql
plot(effect(variables[[9]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Size
plot(effect(variables[[21]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Type.of.ownership
plot(effect(variables[[22]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Sector
plot(effect(variables[[23]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Revenue
plot(effect(variables[[24]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
#Job.Location
plot(effect(variables[[25]], first_glm), ci.style="band", rescale.axis=FALSE, multiline=TRUE, ylab="Avg.Salary.K.", rug=FALSE, main="")
## First glm
other_glm = glm(Avg.Salary.K. ~ job_title_sim + Hourly + Employer.provided   + google_an + Size + Type.of.ownership + Sector + Python + Revenue + sas, family = Gamma, data = Salary_data_Train)
#summary(first_glm)
pred_other_glm= predict(other_glm, newdata= Salary_data_Test)
#Let's know validate the predictions, calculating R^2 in the testing set
# Evaluate performance on the test set
R2_test <- cor(pred_other_glm, Salary_data_Test$Avg.Salary.K.)^2
RMSE_test <- sqrt(mean((pred_other_glm - Salary_data_Test$Avg.Salary.K.)^2))
cat("R-squared on test set:", R2_test, "\n")
cat("RMSE on test set:", RMSE_test, "\n")
## First glm
final_glm = glm(Avg.Salary.K. ~ job_title_sim*Python  + Sector + Revenue*sas , family = Gamma, data = Salary_data_Train)
#summary(first_glm)
pred_final_glm= predict(final_glm, newdata= Salary_data_Test)
#Let's know validate the predictions, calculating R^2 in the testing set
# Evaluate performance on the test set
R2_test <- cor(pred_final_glm, Salary_data_Test$Avg.Salary.K.)^2
RMSE_test <- sqrt(mean((pred_final_glm - Salary_data_Test$Avg.Salary.K.)^2))
cat("R-squared on test set:", R2_test, "\n")
cat("RMSE on test set:", RMSE_test, "\n")
library(sjPlot)
#type pred: Predicted values (marginal effects) for specific model terms
#type int: Marginal effects of interaction terms in model
plot_model(final_glm, type = "pred", terms='Sector')+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot_model(final_glm, type = "int")[[1]]+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
plot_model(final_glm, type = "int")[[2]]+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
##First approach
pred_final_glm= predict(final_glm, newdata= Salary_data_Test, type = "response", se.fit = TRUE)
# Create a data frame for plotting
plot_data <- data.frame(Real = Salary_data_Test$Avg.Salary.K.,
Pred =  pred_final_glm$fit,
Lower = pred_final_glm$fit - 1.96 * pred_final_glm$se.fit,
Upper = pred_final_glm$fit + 1.96 * pred_final_glm$se.fit)
# Prediction Interval Plot
ggplot(plot_data, aes(x = Real, y = Pred)) +
geom_point() +
geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, fill = "blue") +
labs(title = "Scatter Plot of Real vs. Predicted",
x = "Real",
y = "Predicted")
# Counting the points outside the intervals
outside_interval_count <- sum(Salary_data_Test$Avg.Salary.K. < pred_final_glm$fit - 1.96 * pred_final_glm$se.fit | Salary_data_Test$Avg.Salary.K. > pred_final_glm$fit + 1.96 * pred_final_glm$se.fit)
# Calculating the coverage
total_points <- nrow(Salary_data_Test)
coverage <- round(100-(outside_interval_count / total_points) * 100, digits=1)
# Printing the coverage
print(paste("Percentage of points inside the intervals:", coverage, "%"))
## Second approach
# Extract necessary information
fit <- pred_final_glm$fit
se_fit <- pred_final_glm$se.fit
# Calculate prediction intervals using the inverse link function and Gamma distribution properties
lower_limit <- fit * exp(-1.96 * se_fit)
upper_limit <- fit * exp(1.96 * se_fit)
# Create a data frame for plotting
plot_data <- data.frame(
Real = Salary_data_Test$Avg.Salary.K.,
Pred = fit,
Lower = lower_limit,
Upper = upper_limit
)
y_limits <- c(0, 10^18)
ggplot(plot_data, aes(x = Real, y = Pred)) +
geom_point() +
geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, fill = "blue") +
labs(title = "Scatter Plot of Real vs. Predicted",
x = "Real",
y = "Predicted")+  coord_cartesian(ylim = y_limits)
## Second approach
# Extract necessary information
fit <- pred_final_glm$fit
se_fit <- pred_final_glm$se.fit
# Calculate prediction intervals using the inverse link function and Gamma distribution properties
lower_limit <- fit * exp(-1.96 * se_fit)
upper_limit <- fit * exp(1.96 * se_fit)
# Create a data frame for plotting
plot_data <- data.frame(
Real = Salary_data_Test$Avg.Salary.K.,
Pred = fit,
Lower = lower_limit,
Upper = upper_limit
)
y_limits <- c(0, 10^18)
ggplot(plot_data, aes(x = Real, y = Pred)) +
geom_point() +
geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, fill = "blue") +
labs(title = "Scatter Plot of Real vs. Predicted",
x = "Real",
y = "Predicted")+  coord_cartesian(ylim = y_limits)
# Counting the points inside the intervals
inside_interval_count <- sum(Salary_data_Test$Avg.Salary.K. > lower_limit & Salary_data_Test$Avg.Salary.K. < upper_limit)
# Calculating the coverage
total_points <- nrow(Salary_data_Test)
coverage <- round((inside_interval_count / total_points) * 100, digits = 1)
# Printing the coverage
print(paste("Percentage of points inside the intervals:", coverage, "%"))
# Counting the points outside the intervals
outside_interval_count <- sum(Salary_data_Test$Avg.Salary.K. < pred_final_glm$fit - 1.96 * pred_final_glm$se.fit | Salary_data_Test$Avg.Salary.K. > pred_final_glm$fit + 1.96 * pred_final_glm$se.fit)
# Calculating the coverage
total_points <- nrow(Salary_data_Test)
coverage <- round(100-(outside_interval_count / total_points) * 100, digits=1)
# Printing the coverage
print(paste("Percentage of points inside the intervals:", coverage, "%"))
##First approach
pred_final_glm= predict(final_glm, newdata= Salary_data_Test, type = "response", se.fit = TRUE)
# Create a data frame for plotting
plot_data <- data.frame(Real = Salary_data_Test$Avg.Salary.K.,
Pred =  pred_final_glm$fit,
Lower = pred_final_glm$fit - 1.96 * pred_final_glm$se.fit,
Upper = pred_final_glm$fit + 1.96 * pred_final_glm$se.fit)
# Prediction Interval Plot
ggplot(plot_data, aes(x = Real, y = Pred)) +
geom_point() +
geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, fill = "blue") +
labs(title = "Scatter Plot of Real vs. Predicted",
x = "Real",
y = "Predicted")
# Counting the points outside the intervals
outside_interval_count <- sum(Salary_data_Test$Avg.Salary.K. < pred_final_glm$fit - 1.96 * pred_final_glm$se.fit | Salary_data_Test$Avg.Salary.K. > pred_final_glm$fit + 1.96 * pred_final_glm$se.fit)
# Calculating the coverage
total_points <- nrow(Salary_data_Test)
coverage <- round(100-(outside_interval_count / total_points) * 100, digits=1)
# Printing the coverage
print(paste("Percentage of points inside the intervals approach 1 :", coverage, "%"))
##First approach
pred_final_glm= predict(final_glm, newdata= Salary_data_Test, type = "response", se.fit = TRUE)
# Create a data frame for plotting
plot_data <- data.frame(Real = Salary_data_Test$Avg.Salary.K.,
Pred =  pred_final_glm$fit,
Lower = pred_final_glm$fit - 1.96 * pred_final_glm$se.fit,
Upper = pred_final_glm$fit + 1.96 * pred_final_glm$se.fit)
# Prediction Interval Plot
ggplot(plot_data, aes(x = Real, y = Pred)) +
geom_point() +
geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, fill = "blue") +
labs(title = "Scatter Plot of Real vs. Predicted",
x = "Real",
y = "Predicted")
# Counting the points outside the intervals
outside_interval_count <- sum(Salary_data_Test$Avg.Salary.K. < pred_final_glm$fit - 1.96 * pred_final_glm$se.fit | Salary_data_Test$Avg.Salary.K. > pred_final_glm$fit + 1.96 * pred_final_glm$se.fit)
# Calculating the coverage
total_points <- nrow(Salary_data_Test)
coverage <- round(100-(outside_interval_count / total_points) * 100, digits=1)
# Printing the coverage
print(paste("Percentage of points inside the intervals approach 1 :", coverage, "%"))
## Second approach
# Extract necessary information
fit <- pred_final_glm$fit
se_fit <- pred_final_glm$se.fit
# Calculate prediction intervals using the inverse link function and Gamma distribution properties
lower_limit <- fit * exp(-1.96 * se_fit)
upper_limit <- fit * exp(1.96 * se_fit)
# Create a data frame for plotting
plot_data <- data.frame(
Real = Salary_data_Test$Avg.Salary.K.,
Pred = fit,
Lower = lower_limit,
Upper = upper_limit
)
y_limits <- c(0, 10^18)
ggplot(plot_data, aes(x = Real, y = Pred)) +
geom_point() +
geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, fill = "blue") +
labs(title = "Scatter Plot of Real vs. Predicted",
x = "Real",
y = "Predicted")+  coord_cartesian(ylim = y_limits)
# Counting the points inside the intervals
inside_interval_count <- sum(Salary_data_Test$Avg.Salary.K. > lower_limit & Salary_data_Test$Avg.Salary.K. < upper_limit)
# Calculating the coverage
total_points <- nrow(Salary_data_Test)
coverage <- round((inside_interval_count / total_points) * 100, digits = 1)
# Printing the coverage
print(paste("Percentage of points inside the intervals approach 2:", coverage, "%"))
install.packages('pdflatex')
R.version.string
install.packages('MiKTeX')
tinytex::install_tinytex()
library(tidyverse)
library(MASS)
library(caret) #ML tools
library(e1071)
library(ggplot2)
install.packages('htmltools', version=0.5.7)
install.packages('htmltools')
install.packages("htmltools")
install.packages("htmltools")
